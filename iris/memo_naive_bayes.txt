ナイーブベイズ分類器のアルゴリズム
https://qiita.com/ishizakiiii/items/07cc7e463dceb3efe1a1

学習データ：
    カテゴリ：C0, ..., Cl (カテゴリ数=l+1)
    文書：D0, ..., Dm
    各文書はいずれかのカテゴリに排他的に分類されること。

ゴール：
    P(Ci|D)
    試験データで与えられた文書Dがカテゴリ Ci (0≦i≦l) に属する確率。

P(Ci)
    カテゴリ Ci 出現率
    学習データのカテゴリCiの文書数を、総文書数で割った値 (すべてのカテゴリに対して算出)

P(Wj|Ci)
    カテゴリ Ci 内で単語 Wj が出現する確率
    カテゴリ Ci 内の単語 Wj の出現数をカテゴリ Ci 内の全単語の出現数で割った値
    （Wj は対象文書に含まれる単語。すべてのカテゴリかつすべての単語について算出）

    ★学習データには存在しない新ワードがあった場合、カテゴリーの確率が 0 になってしまうので
    これを回避すべく、(カテゴリ Ci 内の単語 Wj の出現数 +1)を(カテゴリ Ci 内の全単語の出現数 + 学習データの全単語数)で
    割った値を使う。なお、学習データの全単語数は重複排除した個数とする。

P(D|Ci)
    カテゴリ Ci 内で文書 D が出現する確率。
    文書 D に出現する各単語の、カテゴリ Ci 内で出現する確率を掛け合わせたもの。
    D := W0 , W1, ..., Wi, ..., Wn
    P(D|Ci) = P(W0|Ci) x ... x P(Wn|Ci)

P(Ci|D)
    文書 D がカテゴリ Ci に属する確率。
    カテゴリー出現率 P(Ci)とカテゴリー内の文書出現率 P(D|Ci) を掛け合わせたものに比例。
　　P(Ci|D) ∝ P(Ci) x P(D|Ci)
             = P(Ci) x P(W0|Ci) x ... x P(Wn|Ci)

★単語数が多いと分母の値が非常に大きくなりアンダーフローが起きる恐れがあるので、
これを回避すべく対数をとる。

    log(P(Ci|D)) ∝ log( P(Ci) x P(D|Ci) )
                 = log( P(Ci) x P(W0|Ci) x ... x P(Wn|Ci) )
                 = log(P(Ci)) + log(P(W0|Ci)) + ... + log(P(Wn|Ci))

    ★最終的に P(C0|D), ..., P(Cl|D) のうち最大のものを推測カテゴリとする。
