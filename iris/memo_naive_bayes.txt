ナイーブベイズ分類器のアルゴリズム
https://qiita.com/ishizakiiii/items/07cc7e463dceb3efe1a1

学習データ：
    カテゴリ：C0, ..., Cl (カテゴリ数=l+1)
    文書：D0, ..., Dm
    各文書はいずれかのカテゴリに排他的に分類されること。

ゴール：
    P(Ci|D)
    試験データで与えられた文書DがカテゴリCi (0≦i≦l) に属する確率。

P(Ci)
    カテゴリCi出現率
    学習データのカテゴリCiの文書数を、総文書数で割った値 (すべてのカテゴリに対して算出)

P(Wj|Ci)
    カテゴリCi内の単語Wjの出現率 カテゴリCi内の単語Wjの出現数をカテゴリCi内の全単語の出現数で割った値
    （Wj は対象文書に含まれる単語。すべてのカテゴリかつすべての単語について算出）

    ★学習データには存在しない新ワードがあった場合、カテゴリーの確率が0になってしまうので
    これを回避すべく、(カテゴリCi内の単語Wjの出現数+1)を(カテゴリCi内の全単語の出現数+学習データの全単語数)で
    割った値を使う。なお、学習データの全単語数は重複排除する。

P(D|Ci)
    カテゴリCi内の文書 D の出現率。カテゴリCi内の全単語の出現率を掛け合わせたもの。
    D := W0 , W1, ..., Wi, ..., Wn
    P(D|Ci) = P(W0|Ci) x ... x P(Wn|Ci)

P(Ci|D)
    文書DkがカテゴリCiに属する確率。
    カテゴリー出現率 P(Ci)とカテゴリー内の文書出現率 P(D|Ci)を掛け合わせたもの
　　P(Ci|D) ∝ P(Ci) x P(D|Ci)
             = P(Ci) x P(W0|Ci) x ... x P(Wn|Ci)
    ★単語数が多いと分母の値が非常に大きくなりアンダーフローが起きる恐れがあるので、
    これを回避すべく対数をとる。

    log(P(Ci|D)) ∝ log( P(Ci) x P(D|Ci) )
                 = log( P(Ci) x P(W0|Ci) x ... x P(Wn|Ci) )
                 = log(P(Ci)) + log(P(W0|Ci)) + ... + log(P(Wn|Ci))

    ★最終的に P(C0|D), ..., P(Cl|D) のうち最大のものを推測カテゴリとする。
